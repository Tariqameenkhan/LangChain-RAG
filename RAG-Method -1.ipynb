{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaoGGkiLVe3pFEko/7OAcS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariqameenkhan/LangChain-RAG/blob/main/RAG-Method%20-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEebt5CTVd05"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-pinecone langchain-google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import getpass\n",
        "# import os\n",
        "# import time\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "# if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "#     os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
        "\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "nCa5ayNRXkJQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "index_name = \"langchain-rag-project2\"  # Changed to lowercase\n",
        "\n",
        "# existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "# if index_name not in existing_indexes:\n",
        "pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "    # while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "    #     time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "if4FIC4vZVqn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "negIZJZjZWSs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "pV3iALUIf0_K"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancake sand scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was chocalate chip pancake. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad  test  chocalate chip pancakes very bad  :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_3,\n",
        "    document_5,\n",
        "    document_8,\n",
        "    document_10,\n",
        "]"
      ],
      "metadata": {
        "id": "ekkp9Qvuf1XI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN2ArKDzhEUf",
        "outputId": "861754c0-51bc-4682-c6cc-7d6901afc081"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['79051c55-6da8-40c0-b240-4552a2dffadf',\n",
              " '671140ee-d661-40e8-a81c-e7113fc2fc63',\n",
              " '8e9a0ea9-ff18-448b-beee-69febb95834c',\n",
              " '595aa208-abcb-4881-b8bc-5b4801b92bb9',\n",
              " 'af38d6f4-ac6f-4880-9498-cec8400286de']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "xxjxxuOJEnCX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive loop for additional user queries\n",
        "while True:\n",
        "    user_input = input(\"Ask something: \")  # Get user input inside the loop\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "    else:\n",
        "        # Perform vector search for the initial query\n",
        "        results = vector_store.similarity_search(user_input, k=1)\n",
        "\n",
        "        # Pass the initial query and results to the model\n",
        "        final_answer = llm.invoke(f\"Answer this user query: {user_input}, here are some references to answer: {results}\")\n",
        "\n",
        "\n",
        "        print(f\"*Initial query answer: {final_answer}  \")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L8b6TRC1XTxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}