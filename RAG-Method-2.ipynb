{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tariqameenkhan/LangChain-RAG/blob/main/RAG-Method-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1cmYcMeMfrck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859da931-1612-4e58-f966-50c139eb6876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/2.5 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langchain-google-genai  langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytRcQMFlfwXm",
        "outputId": "dae4b4ad-d883-4e6f-f0b4-7a94796d69eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m297.0/298.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')\n",
        "userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "9ZohdWSqfysv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure your VertexAI credentials are configured\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EJrcgIUxgbNz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "embeddings.embed_query(\"Hello world\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kkqQDkOJgg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone,ServerlessSpec\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "\n",
        "pc = Pinecone(api_key= api_key, environment='us-east-1')  # Use your Pinecone environment\n",
        "\n",
        "index_name = \"helloworld\"\n",
        "\n",
        "\n",
        "pc.create_index(\n",
        "      name=index_name,\n",
        "      dimension=768, # Replace with your model dimensions\n",
        "      metric=\"cosine\", # Replace with your model metric\n",
        "      spec=ServerlessSpec(\n",
        "          cloud=\"aws\",\n",
        "          region=\"us-east-1\"\n",
        "      )\n",
        "  )\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
      ],
      "metadata": {
        "id": "v_I7oK7Sgkeb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents\n",
        "loader = PyPDFLoader(\"name.pdf\")  # Replace with your file\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "fQnVuH1niCYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f5954b-d02e-421f-ec28-8cf44dbf3593"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'name.pdf', 'page': 0}, page_content='My name is tariq. My name is ameen. My name is khan. My name is zain.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "vectors=[]\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "    # Generate a unique ID for each document (e.g., UUID or hash)\n",
        "    doc_id = str(hash(doc.page_content))\n",
        "    metadata = {\"text\": doc.page_content}\n",
        "    # Upsert the vector with a unique ID\n",
        "    index.upsert(vectors=[(doc_id, vector,metadata)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJSaXhU1nUl_",
        "outputId": "56ac0ec9-3704-41b3-a54a-8720aec8c8ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "retriever=  Pinecone(index=index, embedding=embeddings, text_key=\"text\")\n",
        "\n",
        "\n",
        "# Convert it into a retriever compatible with RetrievalQA\n",
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "lOcTEdWwzUVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e666d5d5-f555-48f6-cc0e-53686c6bce1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b373e6fbbf23>:4: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-pinecone package and should be used instead. To use it run `pip install -U :class:`~langchain-pinecone` and import as `from :class:`~langchain_pinecone import Pinecone``.\n",
            "  retriever=  Pinecone(index=index, embedding=embeddings, text_key=\"text\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",  # Other options: \"map_reduce\", \"refine\"\n",
        "    retriever=retriever\n",
        ")\n"
      ],
      "metadata": {
        "id": "8zI4ydrZzjbF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input = input(\"Ask something: \")\n",
        "  if user_input.lower() == \"quit\":\n",
        "    break\n",
        "  else:\n",
        "    response = qa_chain.run(user_input)\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "6OWlE1Euz4jL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8075f94c-e5fe-4649-9236-483b5c9e6c91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask something: what is name\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-d0d6668f0220>:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa_chain.run(user_input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided text, the names are Tariq, Ameen, Khan, and Zain.\n",
            "Ask something: pakistan founder name\n",
            "The provided text does not contain the name of the founder of Pakistan.\n",
            "Ask something: explain ai\n",
            "Based solely on the provided text, I cannot explain AI.  The text only gives a list of names and provides no information about artificial intelligence.  To explain AI, I would need access to information beyond that text.\n",
            "Ask something: explain my\n",
            "Based solely on the provided text, there is no information about \"my\".  The text only states the names of four people: Tariq, Ameen, Khan, and Zain.  Therefore, I cannot explain \"my\" in relation to this text.\n",
            "Ask something: explain name\n",
            "Based on the provided text, \"name\" refers to a person's identifier.  The text lists four names: Tariq, Ameen, Khan, and Zain.\n",
            "Ask something: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yB4OQ0Z1qFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}